# Product Requirements Document (PRD)

## Title
AI-Powered News Discussion Livestream

## Overview
This system automates the process of curating top news stories from Reddit, filtering them with an LLM for relevance, and generating discussions or commentary by AI hosts (e.g., debaters or podcasters). The resulting content is posted to a livestream, providing users with a real-time feed of AI-generated insights on current news topics. Priorities are denoted as P0 (highest) and P1 (secondary), guiding the development focus.

## Objectives
- Deliver a continuous livestream of AI-generated discussions or commentary on trending news from Reddit.
- Automate news selection and content generation to minimize manual effort.
- Create an engaging experience for users to follow live AI-driven news analysis.

## Functional Requirements

### 1. News Fetching (P0)
- **Description**: Retrieve a list of top news stories from Reddit.
- **Details**:
  - Use the Reddit API to fetch top posts from https://www.reddit.com/r/popular/
  - Configurable parameters:
    - Subreddits to target.
    - Number of stories to fetch (e.g., top 20 posts).
    - Fetch frequency (e.g., hourly).
- **Output**: A list of news stories with titles, summaries, and metadata (e.g., upvotes, timestamps, url).

### 2. News Filtering (P1)
- **Description**: Filter the fetched news stories using an LLM to select the most relevant or engaging ones.
- **Details**:
  - Define filtering criteria (e.g., relevance to specific topics like technology, politics, or general interest).
  - Use an LLM to assign a relevance score or classify stories based on a prompt (e.g., "Rate the relevance of this news story to technology on a scale of 1-10").
  - Filter out NSFW posts.
  - Select a subset of stories (e.g., top 5) based on the filtering results.
- **Output**: A filtered list of news stories ready for processing.

### 3. Queue Management (P0)
- **Description**: Store the filtered news stories in a queue for sequential processing.
- **Details**:
  - Use a database table (e.g., in Supabase) to maintain the queue.
  - Process one news story at a time: complete its discussion before moving to the next.
- **Output**: An ordered queue of news stories awaiting discussion generation.

### 4. Discussion Generation (P0)
- **Description**: Generate prompts and content for AI hosts to discuss or comment on each news story.
- **Details**:
  - **Prompt Creation**:
    - For each news story, create prompts for two AI hosts (e.g., "Introduce this news story and share your thoughts" or "Respond to the previous comment about this news story").
    - Hosts can be styled as debaters (arguing perspectives) or podcasters (commenting conversationally), depending on the desired format.
  - **Conversation Flow**:
    - Limit the discussion to a maximum of 5 turns (messages) per topic.
    - Example structure with two AI hosts:
      1. Host 1: Introduces the news story and gives initial thoughts.
      2. Host 2: Responds to Host 1.
      3. Host 1: Responds to Host 2.
      4. Host 2: Responds to Host 1.
      5. Host 1: Concludes with a final response.
    - Each message is generated by an LLM using the news story and the previous message as context.
  - **Output**: A 5-message conversation for each news story.

### 5. Turn Limitation (P0)
- **Description**: Restrict AI hosts to a maximum of 5 turns per news topic.
- **Details**:
  - Enforce a total of 5 messages per discussion, alternating between two hosts.
  - Ensure the conversation remains concise and focused.
- **Output**: A compact, 5-message discussion thread.

### 6. Livestream Posting (P0/P1)
- **Description**: Post the generated discussion content to a livestream in real-time.
- **Details**:
  - Post each message as it’s generated to simulate a live conversation.
  - Use Supabase’s real-time database features to insert messages into a `livestream_messages` table.
  - Frontend subscribes to this table to display updates instantly.
- **Output**: A continuous livestream of discussion messages viewable by users.

## Non-Functional Requirements
- **Performance**: Generate and post messages with minimal latency (e.g., <10 seconds per message).
- **Reliability**: Gracefully handle API errors (e.g., Reddit or LLM failures) with retries or fallbacks.
- **Scalability**: Support a moderate number of concurrent livestream viewers (e.g., hundreds).
- **Cost Efficiency**: Optimize LLM API usage to control operational costs.

## Technical Stack
- **Frontend**: Vite, React, TypeScript, Tailwind, Shadcn, Framer Motion
- **Backend**: Supabase (database, real-time features, Edge Functions)
- **State Management**: React Query
- **Routing**: React Router
- **APIs**:
  - Reddit API for news fetching.
  - OpenAI API (or similar LLM) for filtering and content generation.

## Implementation Phases

### Phase 1: News Fetching and Filtering
- Integrate Reddit API to fetch top news stories.
- Implement LLM-based filtering to select relevant stories.
- Store filtered stories in a Supabase queue table.

### Phase 2: Discussion Generation
- Develop logic to generate 5-message discussions using LLM prompts.
- Test different host styles (debaters vs. podcasters) to refine the format.
- Validate conversation coherence and quality.

### Phase 3: Livestream Integration
- Set up Supabase real-time database for livestream messages.
- Build a React frontend to display the livestream in a chat-like UI.
- Ensure messages post in real-time as generated.

### Phase 4: Optimization
- Adjust fetch frequency, filtering criteria, and prompt design based on feedback.
- Implement basic error handling and logging.
- Monitor API usage for cost and performance optimization.

## Success Metrics
- **Engagement**: Number of unique livestream viewers and average watch time.
- **Content Quality**: User feedback on discussion relevance and engagement.
- **Operational**: System uptime, error rates, and API cost efficiency.

## Risks and Mitigations
- **API Rate Limits**: Cache Reddit data and batch LLM requests to stay within limits.
- **Content Quality**: Refine prompts and add lightweight moderation (e.g., keyword filters) if needed.
- **Latency**: Test LLM response times and adjust posting intervals if delays are noticeable.

## Future Considerations
- **Audio Integration**: Convert text to speech for a podcast-like audio stream.
- **Interactivity**: Allow users to suggest topics or vote on news stories.
- **Enhanced Filtering**: Use advanced LLM techniques (e.g., summarization) for better story selection.

